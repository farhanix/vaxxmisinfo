{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Modules\n",
    "\n",
    "The programs below will import all of the necessary modules used in the data pre-processing program.  Various functions and libraries will be called throughout the duration of the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install twython\n",
    "from twython import Twython\n",
    "from twython.exceptions import TwythonError, TwythonRateLimitError\n",
    "import io, json, time, os, logging, argparse, atexit, gzip, sys, csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Matt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#insert normal libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import ast\n",
    "import datetime as dt\n",
    "import os.path\n",
    "\n",
    "#insert webscraping tools\n",
    "import requests\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from numpy import int64\n",
    "\n",
    "#insert NLTK libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import *\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Changing Folder Paths\n",
    "\n",
    "The data-preprocessing program has been constructed in such a way where it constantly reads and exports data to your local folder paths.  It is recommended that Google Drive Backup & Sync (GDBS) is installed on your device if you are collaborating with multiple people while using this framework.  GDBS allows for your folders on Google Drive to be accessed locally by the program.\n",
    "\n",
    "Please change you folder path names here.  Each folder path can be described in detail below:\n",
    "\n",
    "**project_folder:** root folder name for all relevant files.<br>\n",
    "**json_path:** folder to hold all exported JSON files from Twython.<br>\n",
    "**converted_path:** deposit initial Excel files.<br>\n",
    "**converted_url_path:** deposit final Excel files.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_folder = r'C:\\Users\\Matt\\Google Drive\\Think Luna' # <-- change root folder name\n",
    "\n",
    "json_path = project_folder + r'\\Data\\02_Json' # <-- change folder location for your stored JSON files\n",
    "converted_path = project_folder + r'\\Data\\converted_to_excel' # <-- change folder location for depositing your Excelfiles\n",
    "\n",
    "converted_url_path = project_folder + r'\\Data\\converted_to_url' # <-- change folder location for depositing your Excelfiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Keywords Filter List\n",
    "\n",
    "Please enter a list of negative and positive keywords here.  These will be used to identify potential negative and positive vaccine sentiments in the database when you are ready to label the training data.  A \"keyword_filter\" column will appear at the end of the database which may speed up the labelling process and help to determine class targets.\n",
    "\n",
    "A list of both sets of keywords has been identified below.  Please add any additional words that may be relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_keywords = [\"hearus\",\"FDA\", \"poisoning\", \"ugly\", \"truth\", \"aborted\", \"tissue\", \"fetal\", \"population\", \"VAERS\",\n",
    "\"africa\", \"fetal\", \"cells\", \"population\", \"control\", \"vaxxed\", \"fetus\", \"profit\", \"vaxxed\", \"force\", \"SIDS\", \n",
    "\"victim\", \"agenda\", \"fraud\", \"sterilization\", \"victims\", \"allergy\", \"free\", \"choice\", \"theft\", \"wake up\", \"aluminium\", \n",
    "\"freedom\", \"thimerosal\", \"wakefield\", \"aluminum\", \"Gates\", \"tissue\", \"warfare\", \"Bill\", \"gilead\", \"toxic\", \"weapon\",\n",
    "\"black\", \"greed\", \"kill\", \"whistleblower\", \"chip\", \"hidden\", \"liars\", \"witnessed\", \"choice\", \"hoax\", \"mandate\", \n",
    "\"compensation\", \"injure\", \"manmade\", \"control\", \"injured\", \"man-made\", \"corrupt\", \"injuries\", \"manufacture\", \"damage\", \n",
    "\"injury\", \"merck\", \"damage\", \"insert\", \"mercury\", \"diabetes\", \"patent\", \"monetize\", \"disclose\", \"Poison\", \"natural\", \"engineer\", \n",
    "\"Patent\", \"truth\"]\n",
    "\n",
    "pos_keywords = [\"flufighters\", \"flujab\", \"vaccineswork\", \"clinic\", \"public health\", \n",
    "                \"idweek19\", \"flu shot\", \"flushot\", \"fluvaccine\", \"flu jab\", \"flu vaccine\", \n",
    "                \"get your flu shot\", \"fluchampions\", \"flu war\", \"flu season\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Extraction\n",
    "\n",
    "This program uses Twitter data structured as a JSON filetype.  The problem with this however is that the data is not structured in the normal JSON format.  There are subsets of dictionaries embedded under certain headings (ie. the \"User\" heading contains multiple pieces of information such as \"user_id\", \"friend_count\" and \"follower_count\" all contained in one cell for one particular tweet).  This section of the program extracts the subsets of data from \"user\", \"coordinates\", and \"place\" headings, and appends it to the primary dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing Subsets of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parses the 'user' heading into its own dataframe\n",
    "def parse_user_data(df):\n",
    "    \n",
    "    #index user column and create blank dataframe\n",
    "    idx = df.columns.get_loc(\"user\")\n",
    "    sub_df = pd.DataFrame()\n",
    "\n",
    "    #individually parse out dictionary from user column\n",
    "    for i in range(0,len(df)):\n",
    "        sub_df = sub_df.append(pd.DataFrame(pd.DataFrame(df.iloc[i,idx]).iloc[0,:]).T)\n",
    "\n",
    "    #reset indices in dataframe and list useful columns to keep\n",
    "    sub_df.reset_index(drop=True, inplace=True)\n",
    "    column_list = ['id','name','screen_name','location','description','url','protected','followers_count','friends_count',\n",
    "                   'listed_count','created_at','favourites_count', 'utc_offset','time_zone', 'geo_enabled', 'verified', \n",
    "                   'statuses_count', 'lang','contributors_enabled']\n",
    "    \n",
    "    #build dataframe containing only useful columns\n",
    "    sub_df = sub_df[column_list]\n",
    "    sub_df.reset_index(drop=True, inplace=True)\n",
    "    sub_df.columns = ['user_' + str(col) for col in sub_df.columns] #append \"user\" in front of all column names\n",
    "    \n",
    "    return sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parses the 'coordinates' heading into its own dataframe\n",
    "def parse_coord_data(df):\n",
    "    idx = df.columns.get_loc(\"coordinates\")\n",
    "    \n",
    "    #replace none types with empty dictionaries\n",
    "    thisdict = str({'type': \"Point\",'coordinates': [\"NA\",\"NA\"]})\n",
    "    df['coordinates'].replace([None], thisdict, inplace=True)\n",
    "\n",
    "    sub_df_long = pd.DataFrame()\n",
    "    sub_df_lat = pd.DataFrame()\n",
    "\n",
    "    for i in range(0,len(df)):\n",
    "        try:\n",
    "            sub_df_long = sub_df_long.append(pd.DataFrame(df.iloc[i,idx]).iloc[0,:])\n",
    "        except:\n",
    "            sub_df_long = sub_df_long.append(pd.DataFrame(ast.literal_eval(df.iloc[i,idx])).iloc[0,:])\n",
    "            \n",
    "    \n",
    "    sub_df_long['long'] = sub_df_long['coordinates']\n",
    "    sub_df_long.drop(['coordinates','type'],axis = 1,inplace = True)\n",
    "    sub_df_long.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    for i in range(0,len(df)):\n",
    "        try:\n",
    "            sub_df_lat = sub_df_lat.append(pd.DataFrame(df.iloc[i,idx]).iloc[1,:])\n",
    "        except:\n",
    "            sub_df_lat = sub_df_lat.append(pd.DataFrame(ast.literal_eval(df.iloc[i,idx])).iloc[1,:])\n",
    "        \n",
    "    sub_df_lat['lat'] = sub_df_lat['coordinates']\n",
    "    sub_df_lat.drop(['coordinates','type'],axis = 1,inplace = True)\n",
    "    sub_df_lat.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    sub_df = pd.concat([sub_df_long,sub_df_lat],axis = 1)\n",
    "    \n",
    "    return sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parses the 'place' heading into its own dataframe\n",
    "def parse_place_data(df):\n",
    "    \n",
    "    idx = df.columns.get_loc(\"place\")\n",
    "    \n",
    "    thisdict = str({'id': 'NA', 'url': 'NA', 'place_type': 'NA', 'name': 'NA', 'full_name': 'NA', 'country_code': 'NA', \n",
    "                'country': 'NA', 'contained_within': [], \n",
    "                'bounding_box': {'type': 'NA', 'coordinates': [[['NA', 'NA'], ['NA', 'NA'], ['NA', 'NA'], ['NA', 'NA']]]}, \n",
    "                'attributes': {}})\n",
    "    df['place'].replace([None], thisdict, inplace=True)\n",
    "    \n",
    "    sub_df = pd.DataFrame()\n",
    "\n",
    "    id_list = []\n",
    "    url_list = []\n",
    "    place_type_list = []\n",
    "    name_list = []\n",
    "    cc_list = []\n",
    "    country_list = []\n",
    "\n",
    "    for i in range(0,len(df)):\n",
    "        \n",
    "        if type(df.iloc[i,idx]) == str:\n",
    "            sub_dict = ast.literal_eval(df.iloc[i,idx])\n",
    "        else:\n",
    "            sub_dict = df.iloc[i,idx]\n",
    "\n",
    "        id_list.append(sub_dict['id'])\n",
    "        url_list.append(sub_dict['url'])\n",
    "        place_type_list.append(sub_dict['place_type'])\n",
    "        name_list.append(sub_dict['name'])\n",
    "        cc_list.append(sub_dict['country_code'])\n",
    "        country_list.append(sub_dict['country'])\n",
    "\n",
    "    sub_df['place_id'] = id_list\n",
    "    sub_df['place_url'] = url_list\n",
    "    sub_df['place_type'] = place_type_list\n",
    "    sub_df['place_full_name'] = name_list\n",
    "    sub_df['place_country_code'] = cc_list\n",
    "    sub_df['place_country'] = country_list\n",
    "    \n",
    "    sub_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return sub_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drops unecessary columns from primary dataframe \n",
    "def drop_columns(df):\n",
    "    drop_list = ['str','in_reply','User','geo','Coordinates','Place','entities',\n",
    "                 'utc','time_zone','enabled','contributors','lang','quoted_status']\n",
    "\n",
    "    for st in df.columns:\n",
    "        try:\n",
    "            if any(ext in st for ext in drop_list):\n",
    "                df.drop(st, axis = 1, inplace = True)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the text column and insert new cleaned data\n",
    "def clean_text(df):\n",
    "    \n",
    "    twitter_list = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    for row in df['full_text']:\n",
    "\n",
    "        twitter_text = re.sub('(@[\\w]+)|(^rt\\s+)|(http[s]:\\/\\/[\\w\\.-\\/]+[\\s])|(http:\\/\\/[\\w\\.-\\/]+[\\s])|(#)|(â)','',row) #Remove html attributes \n",
    "        twitter_text = re.sub(r\"http\\S+\", \"\", twitter_text) #Remove URL\n",
    "        twitter_text = re.sub('([^\\w]+)',' ',twitter_text) #Remove punctuation\n",
    "        twitter_text = re.sub('(\\s)x\\w+|xe2',' ',twitter_text) #Remove emojis\n",
    "        twitter_text = twitter_text.lower() #Lowercase all characters\n",
    "        twitter_tokenizer = RegexpTokenizer(r'\\w+') #Setup tokenizer          \n",
    "        twitter_token = twitter_tokenizer.tokenize(twitter_text) #Tokenize words in string\n",
    "        twitter_text = [word for word in twitter_token if word not in stop_words] #Remove stop words\n",
    "\n",
    "        ps = PorterStemmer()\n",
    "        twitter_text = [ps.stem(word) for word in twitter_text] #Stem \n",
    "        twitter_list.append(' '.join(twitter_text))\n",
    "\n",
    "    df.insert(loc = 3, column = 'clean_text', value = pd.DataFrame(twitter_list))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_col_text(url_text,df):\n",
    "    \n",
    "    twitter_list = []\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    for row in url_text:\n",
    "        twitter_text = re.sub('(@[\\w]+)|(^rt\\s+)|(http[s]:\\/\\/[\\w\\.-\\/]+[\\s])|(http:\\/\\/[\\w\\.-\\/]+[\\s])|(#)|(â)','',row) #Remove html attributes \n",
    "        twitter_text = re.sub(r\"http\\S+\", \"\", twitter_text) #Remove URL\n",
    "        twitter_text = re.sub('([^\\w]+)',' ',twitter_text) #Remove punctuation\n",
    "        twitter_text = re.sub('(\\s)x\\w+|xe2',' ',twitter_text) #Remove emojis\n",
    "        twitter_text = twitter_text.lower() #Lowercase all characters\n",
    "        twitter_tokenizer = RegexpTokenizer(r'\\w+') #Setup tokenizer          \n",
    "        twitter_token = twitter_tokenizer.tokenize(twitter_text) #Tokenize words in string\n",
    "        twitter_text = [word for word in twitter_token if word not in stop_words] #Remove stop words\n",
    "\n",
    "        ps = PorterStemmer()\n",
    "        twitter_text = [ps.stem(word) for word in twitter_text] #Stem \n",
    "        twitter_list.append(' '.join(twitter_text).replace('titl',''))\n",
    "\n",
    "    df.insert(loc = len(df.columns), column = url_text.name + '_cleaned', value = twitter_list)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anon_col (df, cols):\n",
    "    for col_name in cols:\n",
    "        keys = {cats: i for i, cats in enumerate(df[col_name].unique())}\n",
    "        df[col_name] = df[col_name].apply(lambda x: keys[x])\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert JSON to Dataframe (Main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert JSON file to readable dataframe\n",
    "def json_2_df(file_loc):\n",
    "    \n",
    "    #read only json file and convert to dataframe\n",
    "    with open(file_loc, 'r') as myfile:\n",
    "        data = myfile.read()\n",
    "        df = pd.read_json(data, lines = True)\n",
    "        \n",
    "        #parsing data subsets into individual dataframes\n",
    "        sub_df_user = parse_user_data(df)\n",
    "        sub_df_coord = parse_coord_data(df)\n",
    "        sub_df_place = parse_place_data(df)\n",
    "        \n",
    "        #combining all dataframes and renaming columns\n",
    "        df_combined = pd.concat([df, sub_df_user, sub_df_coord, sub_df_place], axis=1)\n",
    "        df_combined.rename(columns={\"user\": \"User\", \"place\": \"Place\", \"coordinates\": \"Coordinates\"}, inplace = True)\n",
    "        \n",
    "        #data cleaning (dropping unecessary columns and cleaning twitter text)\n",
    "        drop_columns(df_combined)\n",
    "        clean_text(df_combined)\n",
    "        \n",
    "        #additional cleanups\n",
    "        df_combined[\"target_label\"] = \"\" #insert blank target label column\n",
    "        df_combined['id'] = df_combined['id'].astype(str) #convert id field to string\n",
    "        \n",
    "        #modify datetime to readable Excel format\n",
    "        df_combined.insert(loc = 1, column = 'time_created_at', \n",
    "                           value = pd.to_datetime(df_combined['created_at']).dt.time) #insert created at time column\n",
    "        df_combined['created_at'] = pd.to_datetime(df_combined['created_at']).dt.date #convert datetime to just date\n",
    "    \n",
    "    return df_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exporting Output Files\n",
    "\n",
    "This program utilizes the functions defined above and automatically creates a specific number of dataframes based on the count of JSON files in the 'downloaded_json_folders' path that has been set up in Google Drive.  Once each dataframe is created, it is then exported to Excel with a unique name and added to the 'converted_to_excel' repository.  This ensures that the user will not have to manually create each dataframe or Excel template (ie. if there are 10 JSON files, the program will automatically create 10 dataframes and Excel output files).\n",
    "\n",
    "These Excel files are then merged into a master file, which is exported to the same folder path.  The program will check to see if the master file exists in location.  If it does, this section is bypassed as it can take 20+ minutes for this entire program section to run.  Please note that if you are using this code, you will need to change the folder paths above.  It is highly recommended that you maintain the naming convention of the files so that you will not need to make any additional changes. \n",
    "\n",
    "**Please Note: This program only utilizes 'coords' and 'place' data specified in their naming convention.  The reason for this is to ignore other potential, irrelevant Excel files that may be stored with this data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell first checks to see if the master consolidated file exists\n",
    "if os.path.exists(converted_path + '\\\\master_twitter_cleaned_data.xlsx') != True:\n",
    "\n",
    "    count_place = 0 #place counter set to 0\n",
    "    count_coords = 0 #coords counter set to 0\n",
    "\n",
    "    df_place = {} #define empty place dictionary\n",
    "    df_coords = {} #define empty coords dictionary\n",
    "\n",
    "    #will loop through all JSON files in folder path\n",
    "    for f in os.listdir(json_path):\n",
    "\n",
    "        #if file contains 'place' in name, convert JSON to df with 'place' naming convention\n",
    "        if 'place' in str(f):\n",
    "            df_place[count_place] = json_2_df(json_path + '\\\\all_tweets_place_' + str(count_place) + '.jsonl')\n",
    "            df_place[count_place].set_index('id',inplace = True)\n",
    "            df_place[count_place].to_excel(converted_path + '\\\\twitter_place_clean_data_' + str(count_place) + '.xlsx',encoding='utf-8-sig')\n",
    "\n",
    "            count_place = count_place + 1\n",
    "        \n",
    "        #if file contains 'coords' in name, convert JSON to df with 'coords' naming convention\n",
    "        elif 'coords' in str(f):\n",
    "            df_coords[count_coords] = json_2_df(json_path + '\\\\all_tweets_coords_' + str(count_coords) + '.jsonl')\n",
    "            df_coords[count_coords].set_index('id',inplace = True)\n",
    "            df_coords[count_coords].to_excel(converted_path + '\\\\twitter_coords_clean_data_' + str(count_coords) + '.xlsx',encoding='utf-8-sig')\n",
    "\n",
    "            count_coords = count_coords + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#This cell first checks to see if the master consolidated file exists\n",
    "#If master consolidated file does not exist, this cell will loop through all dataframes and merge them into the master file\n",
    "\n",
    "if os.path.exists(converted_path + '\\\\master_twitter_cleaned_data.xlsx') != True:\n",
    "    \n",
    "    df = {}\n",
    "    count_place = 0\n",
    "    count_coords = 0\n",
    "\n",
    "    dataframe = list()\n",
    "\n",
    "    for f in os.listdir(converted_path):\n",
    "\n",
    "        if 'place' in str(f) and 'desktop.ini' not in str(f) and '~$' not in str(f):\n",
    "            df[count_place] = pd.read_excel(converted_path + '\\\\twitter_place_clean_data_' + str(count_place) + '.xlsx')\n",
    "            dataframe.append(df[count_place])\n",
    "            count_place += 1\n",
    "\n",
    "        if 'coords' in str(f) and 'desktop.ini' not in str(f) and '~$' not in str(f):\n",
    "            df[count_coords] = pd.read_excel(converted_path + '\\\\twitter_coords_clean_data_' + str(count_coords) + '.xlsx')\n",
    "            dataframe.append(df[count_coords])\n",
    "            count_coords += 1\n",
    "\n",
    "    excel_df = pd.concat(dataframe)\n",
    "    excel_df.to_excel(converted_path + '\\\\master_twitter_cleaned_data.xlsx',encoding='utf-8-sig')\n",
    "\n",
    "else:\n",
    "    excel_df = pd.read_excel(converted_path + '\\\\master_twitter_cleaned_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_df.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 122940, Columns: 38\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>time_created_at</th>\n",
       "      <th>full_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>truncated</th>\n",
       "      <th>display_text_range</th>\n",
       "      <th>source</th>\n",
       "      <th>is_quote_status</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>...</th>\n",
       "      <th>user_statuses_count</th>\n",
       "      <th>long</th>\n",
       "      <th>lat</th>\n",
       "      <th>place_id</th>\n",
       "      <th>place_url</th>\n",
       "      <th>place_type</th>\n",
       "      <th>place_full_name</th>\n",
       "      <th>place_country_code</th>\n",
       "      <th>place_country</th>\n",
       "      <th>target_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1115990344401702914</td>\n",
       "      <td>2019-04-10</td>\n",
       "      <td>14:50:06</td>\n",
       "      <td>Fighting Stigma : Harris to explore plans to b...</td>\n",
       "      <td>fight stigma harri explor plan ban children va...</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 142]</td>\n",
       "      <td>&lt;a href=\"https://dlvrit.com/\" rel=\"nofollow\"&gt;d...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>800115</td>\n",
       "      <td>-73.479357</td>\n",
       "      <td>45.667751</td>\n",
       "      <td>62b78f4b441b9a2f</td>\n",
       "      <td>https://api.twitter.com/1.1/geo/id/62b78f4b441...</td>\n",
       "      <td>city</td>\n",
       "      <td>Varennes</td>\n",
       "      <td>CA</td>\n",
       "      <td>Canada</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1226314915452837888</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>01:20:34</td>\n",
       "      <td>Participated in the “V for Vaccine” flash mob ...</td>\n",
       "      <td>particip v vaccin flash mob today brooklyn ny ...</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 187]</td>\n",
       "      <td>&lt;a href=\"http://instagram.com\" rel=\"nofollow\"&gt;...</td>\n",
       "      <td>False</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>21672</td>\n",
       "      <td>-73.975865</td>\n",
       "      <td>40.683033</td>\n",
       "      <td>011add077f4d2da3</td>\n",
       "      <td>https://api.twitter.com/1.1/geo/id/011add077f4...</td>\n",
       "      <td>city</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>US</td>\n",
       "      <td>United States</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1097900340723310593</td>\n",
       "      <td>2019-02-19</td>\n",
       "      <td>16:46:53</td>\n",
       "      <td>Ty helps combat the spread of the flu by follo...</td>\n",
       "      <td>ty help combat spread flu follow rule like ty ...</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 210]</td>\n",
       "      <td>&lt;a href=\"http://instagram.com\" rel=\"nofollow\"&gt;...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4699</td>\n",
       "      <td>-81.488455</td>\n",
       "      <td>36.851915</td>\n",
       "      <td>3437938e5550bb2b</td>\n",
       "      <td>https://api.twitter.com/1.1/geo/id/3437938e555...</td>\n",
       "      <td>city</td>\n",
       "      <td>Marion</td>\n",
       "      <td>US</td>\n",
       "      <td>United States</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id created_at time_created_at  \\\n",
       "0  1115990344401702914 2019-04-10        14:50:06   \n",
       "1  1226314915452837888 2020-02-09        01:20:34   \n",
       "2  1097900340723310593 2019-02-19        16:46:53   \n",
       "\n",
       "                                           full_text  \\\n",
       "0  Fighting Stigma : Harris to explore plans to b...   \n",
       "1  Participated in the “V for Vaccine” flash mob ...   \n",
       "2  Ty helps combat the spread of the flu by follo...   \n",
       "\n",
       "                                          clean_text  truncated  \\\n",
       "0  fight stigma harri explor plan ban children va...      False   \n",
       "1  particip v vaccin flash mob today brooklyn ny ...      False   \n",
       "2  ty help combat spread flu follow rule like ty ...      False   \n",
       "\n",
       "  display_text_range                                             source  \\\n",
       "0           [0, 142]  <a href=\"https://dlvrit.com/\" rel=\"nofollow\">d...   \n",
       "1           [0, 187]  <a href=\"http://instagram.com\" rel=\"nofollow\">...   \n",
       "2           [0, 210]  <a href=\"http://instagram.com\" rel=\"nofollow\">...   \n",
       "\n",
       "   is_quote_status  retweet_count  ...  user_statuses_count       long  \\\n",
       "0            False              0  ...               800115 -73.479357   \n",
       "1            False              9  ...                21672 -73.975865   \n",
       "2            False              0  ...                 4699 -81.488455   \n",
       "\n",
       "         lat          place_id  \\\n",
       "0  45.667751  62b78f4b441b9a2f   \n",
       "1  40.683033  011add077f4d2da3   \n",
       "2  36.851915  3437938e5550bb2b   \n",
       "\n",
       "                                           place_url  place_type  \\\n",
       "0  https://api.twitter.com/1.1/geo/id/62b78f4b441...        city   \n",
       "1  https://api.twitter.com/1.1/geo/id/011add077f4...        city   \n",
       "2  https://api.twitter.com/1.1/geo/id/3437938e555...        city   \n",
       "\n",
       "  place_full_name place_country_code  place_country target_label  \n",
       "0        Varennes                 CA         Canada          NaN  \n",
       "1        Brooklyn                 US  United States          NaN  \n",
       "2          Marion                 US  United States          NaN  \n",
       "\n",
       "[3 rows x 38 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Master consolidated Excel file attributes\n",
    "print(f\"Rows: {excel_df.shape[0]}, Columns: {excel_df.shape[1]}\")\n",
    "excel_df.reset_index(drop=True, inplace=True)\n",
    "excel_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Webscraping URLs & Re-Appending to Master Dataframe\n",
    "\n",
    "The section below focuses on extracting any URL's posted in the 'full_text' column and reappending that information to the master dataframe.  This is necessary since approximately 22% of Tweets contain a URL pointing to another specific page with vaccine hesitency context.  \n",
    "\n",
    "There are 2 types of URL's to be extracted: direct Tweets and others.  Due to Twitter's privacy policy on protecting Tweets, if the URL is a direct Twitter link, the ID will be extracted and run through the Twython API.  Any other URL's will simply extract the title HTML tag.  In both instances, dataframes will be constructed and reappended to the master dataframe.\n",
    "\n",
    "The process of extracting information from the URL's can take upwards of 7+ hours, and so to avoid this runtime, the program will check to see if the \"Tweet\" and \"Title\" dataframes have been exported to Excel.  If so, it will bypass the extraction code.\n",
    "\n",
    "**Please Note: You must have your own Twitter consumer information in order to run the Twython API.  To protect our own user content, this information has been hidden in a password protected file stored over the cloud.  If you don't have this information, simply sign up with your own Twitter account and request for Twython access.  Once you have this, replace the consumer_info objects with your own consumer keys (there will be prompts telling you where to replace the code below).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting URL Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, extract the unique URL's from the post \n",
    "url_list_full = list()\n",
    "\n",
    "for my_string in excel_df['full_text']:\n",
    "    try:\n",
    "        url_link = re.findall(r'(https?://[^\\s]+)', my_string)[0]\n",
    "    except:\n",
    "        url_link = ''\n",
    "\n",
    "    url_list_full.append(url_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Once the unique URL's have been extracted, convert them to their proper domain URL\n",
    "#This will list every language variation of the same URL\n",
    "\n",
    "if os.path.exists(converted_url_path + '\\\\twitter_place_url_extracted_tweets.xlsx') != True and os.path.exists(converted_url_path + '\\\\twitter_place_url_extracted_titles.xlsx') != True:\n",
    "\n",
    "    converted_url_list_full = list()\n",
    "\n",
    "    for url in url_list_full:\n",
    "\n",
    "        try:\n",
    "            html = requests.get(url)\n",
    "            soup = BeautifulSoup(html.content,\"html.parser\")\n",
    "            new_url = soup.find('link', hreflang = 'en')\n",
    "            #print(new_url)\n",
    "            converted_url_list_full.append(new_url)\n",
    "        except:\n",
    "            converted_url_list_full.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the first listed URL (in english)\n",
    "\n",
    "if os.path.exists(converted_url_path + '\\\\twitter_place_url_extracted_tweets.xlsx') != True and os.path.exists(converted_url_path + '\\\\twitter_place_url_extracted_titles.xlsx') != True:\n",
    "\n",
    "    converted_eng_list_full = list()\n",
    "\n",
    "    for i in range(0,len(converted_url_list_full)):\n",
    "        try:\n",
    "            converted_eng_list_full.append(re.findall(r'(https?://[^\\s]+)', str(converted_url_list_full[i]))[0])\n",
    "        except:\n",
    "            converted_eng_list_full.append('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing Title and Tweet Data from URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(converted_url_path + '\\\\twitter_place_url_extracted_tweets.xlsx') != True and os.path.exists(converted_url_path + '\\\\twitter_place_url_extracted_titles.xlsx') != True:\n",
    "\n",
    "    #For title extraction\n",
    "    title_list = list()\n",
    "    title_url_converted_list = list()\n",
    "    title_unique_url = list()\n",
    "\n",
    "    #For Twitter ID extraction\n",
    "    id_list = list()\n",
    "    id_url_converted_list = list()\n",
    "    id_unique_url = list()\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for eng_url in converted_eng_list_full:\n",
    "\n",
    "        if 'twitter' not in str(eng_url):\n",
    "            try:\n",
    "                html = requests.get(eng_url)\n",
    "                soup = BeautifulSoup(html.content,\"html.parser\")\n",
    "                new_url = soup.find('title')\n",
    "\n",
    "                title_list.append(new_url)\n",
    "                title_url_converted_list.append(eng_url)\n",
    "                title_unique_url.append(url_list_full[count])\n",
    "\n",
    "                count += 1\n",
    "            except:\n",
    "                title_list.append('')\n",
    "                title_url_converted_list.append(eng_url)\n",
    "                title_unique_url.append(url_list_full[count])\n",
    "\n",
    "                count += 1\n",
    "        else:\n",
    "            if any(ext in re.findall(r'(https?://[^\\s]+)',eng_url)[0] for ext in ['video','photo']):\n",
    "                temp = eng_url.split('status/')[-1]\n",
    "                digit = temp.split('/')[-3]\n",
    "            else:  \n",
    "                temp = eng_url.split('status/')[-1]\n",
    "                digit = temp.split('?')[-2]\n",
    "\n",
    "            id_list.append(digit)\n",
    "            id_url_converted_list.append(eng_url)\n",
    "            id_unique_url.append(url_list_full[count])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Title Extraction (url2title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_url</th>\n",
       "      <th>full_url</th>\n",
       "      <th>extracted_titles</th>\n",
       "      <th>extracted_titles_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://t.co/1a916WBsA6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://t.co/L0UCON7g78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://t.co/oYlzZrdwE0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                unique_url full_url extracted_titles extracted_titles_cleaned\n",
       "0  https://t.co/1a916WBsA6      NaN              NaN                      NaN\n",
       "1  https://t.co/L0UCON7g78      NaN              NaN                      NaN\n",
       "2  https://t.co/oYlzZrdwE0      NaN              NaN                      NaN"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''url2title'''\n",
    "if os.path.exists(converted_url_path + '\\\\twitter_place_url_extracted_tweets.xlsx') != True and os.path.exists(converted_url_path + '\\\\twitter_place_url_extracted_titles.xlsx') != True:\n",
    "\n",
    "    df_title = pd.DataFrame()\n",
    "\n",
    "    df_title['unique_url'] = title_unique_url\n",
    "    df_title['full_url'] = title_url_converted_list\n",
    "    df_title['extracted_titles'] = title_list\n",
    "    \n",
    "    df_title = clean_col_text(df_title['extracted_titles'].astype(str),df_title)\n",
    "    df_title.to_excel(converted_url_path + '\\\\twitter_place_url_extracted_titles.xlsx',encoding='utf-8-sig')\n",
    "    \n",
    "else:\n",
    "    df_title = pd.read_excel(converted_url_path + '\\\\twitter_place_url_extracted_titles.xlsx')\n",
    "    df_title.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "    \n",
    "df_title.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twitter Extraction (url2tweetID > id2tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>extracted_tweets</th>\n",
       "      <th>unique_url</th>\n",
       "      <th>full_url</th>\n",
       "      <th>extracted_tweets_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1182230763162477056</td>\n",
       "      <td>#BantryGeneralHospital #YourBestShot Our Quali...</td>\n",
       "      <td>https://t.co/kuVrCHdX5t</td>\n",
       "      <td>https://twitter.com/carolcroke1/status/1182230...</td>\n",
       "      <td>bantrygeneralhospit yourbestshot qualiti patie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                   extracted_tweets  \\\n",
       "0  1182230763162477056  #BantryGeneralHospital #YourBestShot Our Quali...   \n",
       "\n",
       "                unique_url                                           full_url  \\\n",
       "0  https://t.co/kuVrCHdX5t  https://twitter.com/carolcroke1/status/1182230...   \n",
       "\n",
       "                            extracted_tweets_cleaned  \n",
       "0  bantrygeneralhospit yourbestshot qualiti patie...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if tweets and title databases DO NOT EXIST in directory then create\n",
    "if os.path.exists(converted_url_path + '\\\\twitter_place_url_extracted_tweets.xlsx') != True and os.path.exists(converted_url_path + '\\\\twitter_place_url_extracted_titles.xlsx') != True:\n",
    "    \n",
    "    '''url2tweetID'''\n",
    "    extracted_tweets = {'unique_url':id_unique_url,'full_url':id_url_converted_list,'twitter_id':id_list}\n",
    "\n",
    "    df_extracted_tweets = pd.DataFrame(extracted_tweets)\n",
    "    df_extracted_tweets.rename(columns = {\"twitter_id\": \"id\"}, inplace = True)\n",
    "    df_extracted_tweets.id = df_extracted_tweets.id.astype(int64)\n",
    "    \n",
    "    df_id_list = df_extracted_tweets.drop(columns = ['unique_url', 'full_url'])\n",
    "    df_id_list.set_index(\"id\",inplace = True)\n",
    "    df_id_list.to_csv('extracted_tweet_ids.csv',encoding='utf-8-sig')\n",
    "    \n",
    "    '''ID2tweet'''\n",
    "    '''Please put in your own credentials where XXXXXXX is indicated below'''\n",
    "    !python download_tweets_vaxxmisinfo.py -i extracted_tweet_ids.csv -o extracted_tweet_ids_downloaded.jsonl --consumerkey XXXXXXX --consumersecret XXXXXXX --accesstoken XXXXXXX  --accesssecret XXXXXXX\n",
    "    \n",
    "    #read from JSONL in local directory\n",
    "    file_temp = 'extracted_tweet_ids_downloaded.jsonl'\n",
    "    with open(file_temp, 'r') as myfile:\n",
    "            data = myfile.read()\n",
    "            df_tweets_temp = pd.read_json(data, lines = True)\n",
    "    \n",
    "    df_tweets_temp = df_tweets_temp[['id','full_text']]\n",
    "    df_tweets_temp.rename(columns = {\"full_text\": \"extracted_tweets\"}, inplace = True)\n",
    "    \n",
    "    df_tweets = pd.merge(df_tweets_temp,df_extracted_tweets,on ='id',how = 'outer')\n",
    "    clean_col_text(df_tweets.extracted_tweets,df_tweets)\n",
    "    \n",
    "    df_tweets.to_excel(converted_url_path + '\\\\twitter_place_url_extracted_tweets.xlsx',encoding='utf-8-sig')\n",
    "    \n",
    "    #move data from local to GDrive\n",
    "    if os.path.exists(converted_url_path + '\\\\extracted_tweet_ids_downloaded.jsonl') != True and os.path.exists(converted_url_path + '\\\\extracted_tweet_ids.csv') != True:\n",
    "        \n",
    "        os.rename('extracted_tweet_ids_downloaded.jsonl', converted_url_path + '\\\\extracted_tweet_ids_downloaded.jsonl' )\n",
    "        os.rename('extracted_tweet_ids.csv', converted_url_path + '\\\\extracted_tweet_ids.csv' )\n",
    "\n",
    "#if tweets and title databases do exist in directory then read from path\n",
    "else:\n",
    "    df_tweets = pd.read_excel(converted_url_path + '\\\\twitter_place_url_extracted_tweets.xlsx')\n",
    "    df_tweets.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "\n",
    "df_tweets.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging Data Frames (url2text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''url2text'''\n",
    "#add unique url to main dataframe and convert type to string\n",
    "excel_df['unique_url'] = url_list_full\n",
    "excel_df['unique_url'] = excel_df['unique_url'].astype(str)\n",
    "\n",
    "#drop all columns from title df except for unique url and extracted clean titles\n",
    "df_title = df_title[['unique_url','extracted_titles_cleaned']]\n",
    "df_title['unique_url'] = df_title['unique_url'].astype(str)\n",
    "\n",
    "#drop all columns from tweets df except for unique url and extracted clean tweets\n",
    "df_tweets = df_tweets[['unique_url','extracted_tweets_cleaned']]\n",
    "df_tweets['unique_url'] = df_tweets['unique_url'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new empty df \n",
    "df = pd.DataFrame()\n",
    "\n",
    "#merge all df's on unique url and drop duplicates\n",
    "df = pd.merge(excel_df,df_title,on = 'unique_url', how = 'left')\n",
    "df_final = df.merge(df_tweets, on = 'unique_url', how = 'left').drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset index and fill NA's with blanks\n",
    "df_final.reset_index(drop = True, inplace = True)\n",
    "df_final['id'] = df_final['id'].astype(str)\n",
    "df_final.set_index('id', inplace=True)\n",
    "df_final = df_final.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_list = list()\n",
    "\n",
    "for tweet in df_final['full_text']:\n",
    "    \n",
    "    if any(ele in tweet for ele in neg_keywords) and any(ele in tweet for ele in pos_keywords):\n",
    "        keyword_list.append(\"negative\")\n",
    "    elif any(ele in tweet for ele in neg_keywords):\n",
    "        keyword_list.append(\"negative\")\n",
    "    elif any(ele in tweet for ele in pos_keywords):\n",
    "        keyword_list.append(\"positive\")\n",
    "    else:\n",
    "        keyword_list.append(\"none\")\n",
    "\n",
    "df_final['keywords'] = keyword_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export master annotation file to directory\n",
    "df_final['X'] = df_final['clean_text'] + ' ' + df_final['extracted_titles_cleaned'] + ' ' + df_final['extracted_tweets_cleaned'] + ' ' + df_final['user_screen_name']  \n",
    "df_final = anon_col(df_final,['user_name', 'user_screen_name']) #anonamize Twitter usernames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>time_created_at</th>\n",
       "      <th>full_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>truncated</th>\n",
       "      <th>display_text_range</th>\n",
       "      <th>source</th>\n",
       "      <th>is_quote_status</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>...</th>\n",
       "      <th>place_type</th>\n",
       "      <th>place_full_name</th>\n",
       "      <th>place_country_code</th>\n",
       "      <th>place_country</th>\n",
       "      <th>target_label</th>\n",
       "      <th>unique_url</th>\n",
       "      <th>extracted_titles_cleaned</th>\n",
       "      <th>extracted_tweets_cleaned</th>\n",
       "      <th>keywords</th>\n",
       "      <th>X</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1178908412794434048</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>05:44:03</td>\n",
       "      <td>Stigmabase —  Hope for HIV Vaccine Being Teste...</td>\n",
       "      <td>stigmabas hope hiv vaccin test south africa hi...</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 241]</td>\n",
       "      <td>&lt;a href=\"https://dlvrit.com/\" rel=\"nofollow\"&gt;d...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>city</td>\n",
       "      <td>Johannesburg</td>\n",
       "      <td>ZA</td>\n",
       "      <td>South Africa</td>\n",
       "      <td></td>\n",
       "      <td>https://t.co/1a916WBsA6</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>none</td>\n",
       "      <td>stigmabas hope hiv vaccin test south africa hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178922253167149056</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>06:39:03</td>\n",
       "      <td>Stigmabase —  New research reveals over half o...</td>\n",
       "      <td>stigmabas new research reveal half peopl irela...</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 278]</td>\n",
       "      <td>&lt;a href=\"https://dlvrit.com/\" rel=\"nofollow\"&gt;d...</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>city</td>\n",
       "      <td>Dublin City</td>\n",
       "      <td>IE</td>\n",
       "      <td>Ireland</td>\n",
       "      <td></td>\n",
       "      <td>https://t.co/L0UCON7g78</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>positive</td>\n",
       "      <td>stigmabas new research reveal half peopl irela...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178951214857196032</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>08:34:08</td>\n",
       "      <td>Thoughts? San Francisco:: Tennessee raccoons t...</td>\n",
       "      <td>thought san francisco tennesse raccoon get vac...</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 97]</td>\n",
       "      <td>&lt;a href=\"https://dlvrit.com/\" rel=\"nofollow\"&gt;d...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>city</td>\n",
       "      <td>Portland</td>\n",
       "      <td>US</td>\n",
       "      <td>United States</td>\n",
       "      <td></td>\n",
       "      <td>https://t.co/oYlzZrdwE0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>none</td>\n",
       "      <td>thought san francisco tennesse raccoon get vac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    created_at time_created_at  \\\n",
       "id                                               \n",
       "1178908412794434048 2019-10-01        05:44:03   \n",
       "1178922253167149056 2019-10-01        06:39:03   \n",
       "1178951214857196032 2019-10-01        08:34:08   \n",
       "\n",
       "                                                             full_text  \\\n",
       "id                                                                       \n",
       "1178908412794434048  Stigmabase —  Hope for HIV Vaccine Being Teste...   \n",
       "1178922253167149056  Stigmabase —  New research reveals over half o...   \n",
       "1178951214857196032  Thoughts? San Francisco:: Tennessee raccoons t...   \n",
       "\n",
       "                                                            clean_text  \\\n",
       "id                                                                       \n",
       "1178908412794434048  stigmabas hope hiv vaccin test south africa hi...   \n",
       "1178922253167149056  stigmabas new research reveal half peopl irela...   \n",
       "1178951214857196032  thought san francisco tennesse raccoon get vac...   \n",
       "\n",
       "                     truncated display_text_range  \\\n",
       "id                                                  \n",
       "1178908412794434048      False           [0, 241]   \n",
       "1178922253167149056      False           [0, 278]   \n",
       "1178951214857196032      False            [0, 97]   \n",
       "\n",
       "                                                                source  \\\n",
       "id                                                                       \n",
       "1178908412794434048  <a href=\"https://dlvrit.com/\" rel=\"nofollow\">d...   \n",
       "1178922253167149056  <a href=\"https://dlvrit.com/\" rel=\"nofollow\">d...   \n",
       "1178951214857196032  <a href=\"https://dlvrit.com/\" rel=\"nofollow\">d...   \n",
       "\n",
       "                     is_quote_status  retweet_count  favorite_count  ...  \\\n",
       "id                                                                   ...   \n",
       "1178908412794434048            False              0               0  ...   \n",
       "1178922253167149056            False              1               0  ...   \n",
       "1178951214857196032            False              0               0  ...   \n",
       "\n",
       "                     place_type  place_full_name place_country_code  \\\n",
       "id                                                                    \n",
       "1178908412794434048        city     Johannesburg                 ZA   \n",
       "1178922253167149056        city      Dublin City                 IE   \n",
       "1178951214857196032        city         Portland                 US   \n",
       "\n",
       "                     place_country  target_label               unique_url  \\\n",
       "id                                                                          \n",
       "1178908412794434048   South Africa                https://t.co/1a916WBsA6   \n",
       "1178922253167149056        Ireland                https://t.co/L0UCON7g78   \n",
       "1178951214857196032  United States                https://t.co/oYlzZrdwE0   \n",
       "\n",
       "                     extracted_titles_cleaned extracted_tweets_cleaned  \\\n",
       "id                                                                       \n",
       "1178908412794434048                                                      \n",
       "1178922253167149056                                                      \n",
       "1178951214857196032                                                      \n",
       "\n",
       "                     keywords  \\\n",
       "id                              \n",
       "1178908412794434048      none   \n",
       "1178922253167149056  positive   \n",
       "1178951214857196032      none   \n",
       "\n",
       "                                                                     X  \n",
       "id                                                                      \n",
       "1178908412794434048  stigmabas hope hiv vaccin test south africa hi...  \n",
       "1178922253167149056  stigmabas new research reveal half peopl irela...  \n",
       "1178951214857196032  thought san francisco tennesse raccoon get vac...  \n",
       "\n",
       "[3 rows x 42 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.to_excel(converted_url_path + '\\\\master_twitter_database.xlsx',encoding='utf-8-sig')\n",
    "df_final.head(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
