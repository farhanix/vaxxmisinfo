{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the second code file in the vaxxmisinfo project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Important notes before you start: </b> \n",
    "<br><br>\n",
    "Please make sure to modify the <b>  file paths </b> below to your project folder.\n",
    "<br>\n",
    "Please make sure to modify the <b>  value of the undersampling  </b> to ensure that the correct sampling value is used. \n",
    "<br>\n",
    "<b> Pertaining to model training in section 4:</b> \n",
    "<br><br>\n",
    "Please note that this may take a long time (depending on how many trianing samples you have). The function <b> best_model_search() </b> which trains the models currently uses n_jobs = -1 which utilizes all the cores / threads of your computer's CPU.Should you wish to not allocate the whole CPU towards model training (which is recommended), please adjust the n_jobs to be equal to the number of threads that you can sufficiently allocate. for example, if you have a CPU with 4 threads, you may want to allocate only half of these to the training process, hence you would set n_jobs = 2. \n",
    "<br><br>\n",
    "To check how many threads you have please check \"Task Manager\" on Windows OS devices, or \"About this Mac\" on Mac OS devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import pickle\n",
    "import matplotlib\n",
    "from numpy import int64\n",
    "from collections import Counter\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### File paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_folder = r'C:\\Users\\Farhan\\Google Drive\\Vaxxmissinfoproject'\n",
    "\n",
    "#This is the path where all your annotated data files are\n",
    "data_path = project_folder + '\\Qualitative annotation\\combined\\Master' #<-Modify\n",
    "\n",
    "#The path where the .sav files will be dumped as the models get trained\n",
    "model_path = project_folder + '\\Model Dumps testing' #<-Modify\n",
    " \n",
    "#The path where the the final files with predictions for the dashboard are outputted. \n",
    "dashboard_file_path = project_folder + '\\Dashboard\\pred_data'  #<-Modify\n",
    "\n",
    "#This is the path to the annotated data\n",
    "annotated_data_path = project_folder +'\\Output_Data_Files\\converted_to_url\\master_twitter_database.xlsx'  #<-Modify\n",
    "\n",
    "#This is the path to the best selected model for final model deployment. \n",
    "#This will need to be updated based on your model's results. \n",
    "#Default is M3's Logistic Regression using TFIDF (based on best F1 Score)\n",
    "best_model = model_path + '\\m3\\model_logistic_regression_tfidf_f1_macro.sav'  #<-Modify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sampling values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sampling values. Defaulted to 200\n",
    "\n",
    "m1_sampling = 200  #<-Modify\n",
    "m2_sampling = 200  #<-Modify\n",
    "m3_sampling = 200  #<-Modify\n",
    "m4_sampling = 200  #<-Modify\n",
    "m5_sampling = 200  #<-Modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used for webscraping\n",
    "import re\n",
    "import requests\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SKLearn packages\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split # Splitting Data Sets\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.base import TransformerMixin \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifiers\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLTK \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import *\n",
    "nltk.download(\"stopwords\")\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_start_time = time.time() #This is so that you can see how long the program took (for troubleshooting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Defining Global Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### modify n_jobs if needed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify n_jobs if needed\n",
    "\n",
    "def best_model_search(X, y, vec1, vec2, model_params,model_dump_path):\n",
    "    scores = list()\n",
    "    estimators = list()\n",
    "    vect = None\n",
    "\n",
    "    for vec in [vec1,vec2]:\n",
    "        X = X \n",
    "        y = y\n",
    "        X_ft = vec.fit_transform(X).toarray()\n",
    "\n",
    "        if vec == bow:\n",
    "            vect = \"bag-of-words\"\n",
    "        else:\n",
    "            vect = \"tfidf\"\n",
    "\n",
    "        for measure in ['accuracy','f1_macro','precision_macro','recall_macro']:\n",
    "            for model_name, mp in model_params.items():\n",
    "                print ('Current Model:', model_name, ' with ', vect, ' measure: ', measure)\n",
    "                clf =  GridSearchCV(mp['model'], mp['params'], cv=7, return_train_score=True, scoring = measure, verbose=10, n_jobs =-1)\n",
    "                clf.fit(X_ft, y)\n",
    "                scores.append({\n",
    "                    'model': model_name,\n",
    "                    'vect': vect,\n",
    "                    'score_method': measure,\n",
    "                    'best_score': clf.best_score_,\n",
    "                    'best_params': clf.best_params_\n",
    "                })\n",
    "                print ('Finished Model:', model_name, ' with ', vect, ' measure: ', measure,'best score: ', clf.best_score_)\n",
    "                \n",
    "                \n",
    "                \n",
    "                filename = model_dump_path + '\\\\model_' + model_name + '_' + vect + '_' + measure +'.sav'\n",
    "                #pickle.dump(clf, open(filename, 'wb'))\n",
    "                pickle.dump(clf.best_estimator_, open(filename, 'wb'))\n",
    "                if measure == 'accuracy':\n",
    "                    estimators.append(clf.best_estimator_)\n",
    "\n",
    "            df = pd.DataFrame(scores,columns=['model','vect','score_method','best_score','best_params'])\n",
    "    return df, estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_col_text(url_text,df):\n",
    "    \n",
    "    twitter_list = []\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    for row in url_text:\n",
    "        twitter_text = re.sub('(@[\\w]+)|(^rt\\s+)|(http[s]:\\/\\/[\\w\\.-\\/]+[\\s])|(http:\\/\\/[\\w\\.-\\/]+[\\s])|(#)|(â)','',row) #Remove html attributes \n",
    "        twitter_text = re.sub(r\"http\\S+\", \"\", twitter_text) #Remove URL\n",
    "        twitter_text = re.sub('([^\\w]+)',' ',twitter_text) #Remove punctuation\n",
    "        twitter_text = re.sub('(\\s)x\\w+|xe2',' ',twitter_text) #Remove emojis\n",
    "        twitter_text = twitter_text.lower() #Lowercase all characters\n",
    "        twitter_tokenizer = RegexpTokenizer(r'\\w+') #Setup tokenizer          \n",
    "        twitter_token = twitter_tokenizer.tokenize(twitter_text) #Tokenize words in string\n",
    "        twitter_text = [word for word in twitter_token if word not in stop_words] #Remove stop words\n",
    "\n",
    "        ps = PorterStemmer()\n",
    "        twitter_text = [ps.stem(word) for word in twitter_text] #Stem \n",
    "        twitter_list.append(' '.join(twitter_text).replace('titl',''))\n",
    "\n",
    "    df.insert(loc = len(df.columns), column = url_text.name + '_cleaned', value = twitter_list)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersample_majority_classes(df, us_count):\n",
    "\n",
    "    if df.target_label.nunique() == 5:\n",
    "        \n",
    "        df_class_0 = df[df['target_label'] == 0]\n",
    "        df_class_1 = df[df['target_label'] == 1]\n",
    "        df_class_2 = df[df['target_label'] == 2]\n",
    "        df_class_3 = df[df['target_label'] == 3]\n",
    "        df_class_4 = df[df['target_label'] == 4]\n",
    "        count_class_0 = len(df_class_0.index)\n",
    "        count_class_1 = len(df_class_1.index)\n",
    "        count_class_2 = len(df_class_2.index)\n",
    "        count_class_3 = len(df_class_3.index)\n",
    "        count_class_4 = len(df_class_4.index)\n",
    "\n",
    "        #print('Number of 0 labels', count_class_0)\n",
    "        #print('Number of 1 labels', count_class_1)\n",
    "        #print('Number of 2 labels', count_class_2)\n",
    "        #print('Number of 3 labels', count_class_3)\n",
    "        #print('Number of 4 labels', count_class_4)\n",
    "        #df.target_label.value_counts().plot(kind='bar', title='Count (target)');\n",
    "\n",
    "        df_class_0_under = df_class_0.sample(us_count)\n",
    "\n",
    "        df_test_under = pd.concat([df_class_0_under, df_class_1], axis=0)\n",
    "\n",
    "        print('Random under-sampling:')\n",
    "        #df_test_under.target_label.value_counts().plot(kind='bar', title='Count (target)');\n",
    "        df1 = pd.concat([df_class_0_under, df_class_1], axis=0)\n",
    "        df1 = pd.concat([df1, df_class_2], axis=0) \n",
    "        df1 = pd.concat([df1, df_class_3], axis=0) \n",
    "        df1 = pd.concat([df1, df_class_4], axis=0) \n",
    "        print('Total rows in df: ',len(df1.index))\n",
    "        print (df1.target_label.value_counts().sort_index(axis=0))\n",
    "        df1.target_label.value_counts().plot(kind='bar', title='Count (target)');\n",
    "\n",
    "    elif df.target_label.nunique() == 4:\n",
    "        df_class_0 = df[df['target_label'] == 0]\n",
    "        df_class_1 = df[df['target_label'] == 1]\n",
    "        df_class_2 = df[df['target_label'] == 2]\n",
    "        df_class_3 = df[df['target_label'] == 3]\n",
    "        \n",
    "        count_class_0 = len(df_class_0.index)\n",
    "        count_class_1 = len(df_class_1.index)\n",
    "        count_class_2 = len(df_class_2.index)\n",
    "        count_class_3 = len(df_class_3.index)\n",
    "        \n",
    "        df_class_0_under = df_class_0.sample(us_count)\n",
    "\n",
    "        df_test_under = pd.concat([df_class_0_under, df_class_1], axis=0)\n",
    "\n",
    "        print('Random under-sampling:')\n",
    "       \n",
    "        df1 = pd.concat([df_class_0_under, df_class_1], axis=0)\n",
    "        df1 = pd.concat([df1, df_class_2], axis=0) \n",
    "        df1 = pd.concat([df1, df_class_3], axis=0) \n",
    "        print('Total rows in df: ',len(df1.index))\n",
    "        print (df1.target_label.value_counts().sort_index(axis=0))\n",
    "        df1.target_label.value_counts().plot(kind='bar', title='Count (target)');\n",
    "    \n",
    "    elif df.target_label.nunique() == 3:\n",
    "        df_class_0 = df[df['target_label'] == 0]\n",
    "        df_class_1 = df[df['target_label'] == 1]\n",
    "        df_class_2 = df[df['target_label'] == 2]\n",
    "        \n",
    "        count_class_0 = len(df_class_0.index)\n",
    "        count_class_1 = len(df_class_1.index)\n",
    "        count_class_2 = len(df_class_2.index)\n",
    "        \n",
    "        df_class_0_under = df_class_0.sample(us_count)\n",
    "\n",
    "        df_test_under = pd.concat([df_class_0_under, df_class_1], axis=0)\n",
    "\n",
    "        print('Random under-sampling:')\n",
    "        \n",
    "        df1 = pd.concat([df_class_0_under, df_class_1], axis=0)\n",
    "        df1 = pd.concat([df1, df_class_2], axis=0) \n",
    "        print('Total rows in df: ',len(df1.index))\n",
    "        print (df1.target_label.value_counts().sort_index(axis=0))\n",
    "        df1.target_label.value_counts().plot(kind='bar', title='Count (target)');\n",
    "        \n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram_data(dataframe):\n",
    "    a4_dims = (10, 5)\n",
    "    fig, ax = plt.subplots(figsize=a4_dims)\n",
    "    plt.xticks(rotation=45)\n",
    "    sb.barplot(x='model', y='best_score',\n",
    "    data=dataframe,hue = 'score_method').set_title(f\"Model Comparison by {dataframe.iloc[0,1]}\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to remove any stopwords from the feature set, add then here seperated by a comma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_stopwords = ['amp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_X(X):\n",
    "    \n",
    "    twitter_list = []\n",
    "    stop_words = nltk.corpus.stopwords.words('english') + additional_stopwords\n",
    "\n",
    "    for row in X:\n",
    "        twitter_text = re.sub('(@[\\w]+)|(^rt\\s+)|(http[s]:\\/\\/[\\w\\.-\\/]+[\\s])|(http:\\/\\/[\\w\\.-\\/]+[\\s])|(#)|(â)','',row) #Remove html attributes \n",
    "        twitter_tokenizer = RegexpTokenizer(r'\\w+') #Setup tokenizer          \n",
    "        twitter_token = twitter_tokenizer.tokenize(twitter_text) #Tokenize words in string\n",
    "        twitter_text = [word for word in twitter_token if word not in stop_words] #Remove stop words\n",
    "        twitter_list.append(' '.join(twitter_text).replace('titl',''))\n",
    "\n",
    "    return twitter_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_words (text):\n",
    "    \"\"\"This function converts sentences into a list of words\"\"\"\n",
    "    words=[]\n",
    "\n",
    "    for sentence in text:\n",
    "        wordList = re.sub(\"[^\\w]\", \" \",  sentence).split() #converts each text line into a list of words (creates a nested list)\n",
    "        words.append(wordList)\n",
    "    words= [item for sublist in words for item in sublist] #converts nested list into a 1D list to ease future implementation   \n",
    "        \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_freq_words(words):\n",
    "    counts = dict(Counter(words).most_common(70))\n",
    "\n",
    "    labels, values = zip(*counts.items())\n",
    "\n",
    "    # sort values in descending order\n",
    "    indSort = np.argsort(values)[::-1]\n",
    "\n",
    "    # rearrange data\n",
    "    labels = np.array(labels)[indSort]\n",
    "    values = np.array(values)[indSort]\n",
    "\n",
    "    indexes = np.arange(len(labels))\n",
    "\n",
    "    bar_width = 0.1\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.bar(indexes, values)\n",
    "    plt.xticks(indexes + bar_width, labels,rotation='vertical')\n",
    "    plt.title('Word counts for top 70 most repeated words')\n",
    "    #sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "    plt.savefig(model_path + '/fig_most_freq_words.png', dpi=300)\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_wordcloud(words):\n",
    "    words_str = ' '.join(words) \n",
    "    wordcloud = WordCloud(background_color='white', width = 2000, height = 1000).generate(words_str)\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title('Keywords')\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(model_path + '/fig_word_cloud.png', dpi=300)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anon_col (df, cols):\n",
    "    for col_name in cols:\n",
    "        keys = {cats: i for i, cats in enumerate(df[col_name].unique())}\n",
    "        df[col_name] = df[col_name].apply(lambda x: keys[x])\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Defining global ML training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorization\n",
    "bow = CountVectorizer(stop_words = \"english\", max_features= 300, ngram_range= (1,2)) #bag-of-words\n",
    "tfidf = TfidfVectorizer(stop_words = \"english\", max_df=0.7, max_features= 300, ngram_range= (1,2)) #TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are all the different classifiers and their hyper-parameters which will be used to train on. Modify as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#''\n",
    "model_params = {\n",
    "    'svm': {\n",
    "        'model': svm.SVC(gamma='auto'),\n",
    "        'params' : {\n",
    "            'C': [1,10,20,30],\n",
    "            'kernel': ['rbf','linear']\n",
    "        }  \n",
    "    },\n",
    "    'logistic_regression' : {\n",
    "        'model': LogisticRegression(solver='lbfgs',multi_class='multinomial'),\n",
    "        'params': {\n",
    "            'C': [1,5,10]\n",
    "        }\n",
    "    },\n",
    "    'naive_bayes' : {\n",
    "        'model': MultinomialNB(),\n",
    "        'params': {\n",
    "            'alpha': [0,1]\n",
    "        }\n",
    "    },\n",
    "    'xgboost' : {\n",
    "        'model': xgb.XGBClassifier(),\n",
    "        'params' : {\n",
    "            'max_depth ':[10,20,50,100],\n",
    "            'min_child_weight ':[5,10,15]\n",
    "            \n",
    "        }\n",
    "        \n",
    "    },\n",
    "    'decision_tree' :{\n",
    "        'model' : DecisionTreeClassifier(),\n",
    "        'params' : {\n",
    "            'max_depth' : [10,20,30,100],\n",
    "            'criterion' : ['gini', 'entropy']\n",
    "        }\n",
    "    },\n",
    "    'random_forrest' :{\n",
    "        'model' : RandomForestClassifier(),\n",
    "        'params' : {\n",
    "            'n_estimators' :[50, 100, 200],\n",
    "            'max_depth' : [5, 10, 20, 100]\n",
    "        }\n",
    "    },\n",
    "    'adaboost' : {\n",
    "        'model' : AdaBoostClassifier(),\n",
    "        'params' : {\n",
    "            'n_estimators' :[200]\n",
    "        }\n",
    "    },\n",
    "    'sgd' : {\n",
    "        'model' : SGDClassifier(),\n",
    "        'params' : {\n",
    "            'loss' :['hinge'],\n",
    "            'penalty': ['l2'],\n",
    "            'max_iter' :[5,10,15]\n",
    "        }\n",
    "    },\n",
    "    'k_nearest_neighbours' : {\n",
    "        'model': KNeighborsClassifier(),\n",
    "        'params': {\n",
    "            'n_neighbors': [1,5,10]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "#''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#This is a test param set. to be used when troubleshooting the notebook.\n",
    "\n",
    "model_params = {\n",
    "    'logistic_regression' : {\n",
    "        'model': LogisticRegression(solver='lbfgs',multi_class='multinomial'),\n",
    "        'params': {\n",
    "            'C': [1]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating the Main Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The annotated data will be imported. Each model will use this annotated data (dropping any non labelled Tweets).\n",
    "For each model, a new DF will be created. e.g.: for Model 1: df_m1, for Model 2, df_m2 and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_data = pd.read_excel(annotated_data_path) #This is the file with the annotated data\n",
    "annotated_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m1 = annotated_data\n",
    "df_m1.target_label.astype(str)\n",
    "df_m1 = df_m1[df_m1.target_label != 'TIE BREAK']\n",
    "df_m1 = df_m1[df_m1.target_label != 5]\n",
    "df_m1['target_label'] = df_m1['target_label'].fillna('')\n",
    "df_m1 = df_m1[df_m1.target_label != '']\n",
    "\n",
    "print(df_m1.target_label.value_counts().sort_index(axis=0))\n",
    "\n",
    "print(\"Total samples: \", df_m1.target_label.value_counts().sum())\n",
    "print(\"DF Shape: \",df_m1.shape)\n",
    "\n",
    "X = df_m1.X\n",
    "y = df_m1['target_label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word cloud on the cleaned text file\n",
    "words = text_to_words(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show most frequent words in dataset\n",
    "most_freq_words(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = clean_X(X)\n",
    "words = text_to_words(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show most frequent words in dataset\n",
    "most_freq_words(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(18.5, 10.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_wordcloud(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Important note before you start: </b> \n",
    "<br>\n",
    "\n",
    "Before model training is started, please note that this may take a long time (depending on how many trianing samples you have). The function <b> best_model_search() </b> which trains the models currently uses n_jobs = -1 which utilizes all the cores / threads of your computer's CPU. \n",
    "<br><br>\n",
    "Should you wish to not allocate the whole CPU towards model training (which is recommended), please adjust the n_jobs to be equal to the number of threads that you can sufficiently allocate. for example, if you have a CPU with 4 threads, you may want to allocate only half of these to the training process, hence you would set n_jobs = 2. \n",
    "<br><br>\n",
    "To check how many threads you have please check \"Task Manager\" on Windows OS devices, or \"About this Mac\" on Mac OS devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Model 1\n",
    "##### All classes with random undersampling of class 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Undersampling the data based on the majority class\n",
    "df_m1 = undersample_majority_classes(df_m1, m1_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define feature and target variables\n",
    "X = df_m1.X\n",
    "X = clean_X(X)\n",
    "y = df_m1['target_label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_dump_path = model_path + '\\m1'\n",
    "\n",
    "if not os.path.exists(model_dump_path):\n",
    "    os.mkdir(model_dump_path)\n",
    "    \n",
    "start_time = time.time()\n",
    "\n",
    "df_class, undersampling_est = best_model_search(X, y, bow, tfidf, model_params,model_dump_path)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for vec in ['bag-of-words','tfidf']:\n",
    "    df_score = df_class[df_class['vect'] == vec]\n",
    "    plot_histogram_data(df_score)\n",
    "    df_score.to_excel(model_dump_path + '/scores_' + vec + 'df_score_m1.xlsx',encoding = 'utf-8-sig')\n",
    "    plt.savefig(model_dump_path + '/fig_' + vec + '_results_m1.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ft = bow.fit_transform(X).toarray()\n",
    "y_pred = cross_val_predict(undersampling_est[1], X_ft, y, cv=10)\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "ax = plt.axes()\n",
    "df_cm = sb.heatmap(pd.DataFrame(cm),cmap='Blues',annot=True,ax=ax)\n",
    "ax.set_title('Best Model - Model 1')\n",
    "plt.savefig(model_dump_path + '/fig_' + vec + '_results_m1_CM.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Model 2\n",
    "##### Removing class 4, random undersampling of class 0. Total of four classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m2 = annotated_data\n",
    "df_m2.target_label.astype(str)\n",
    "df_m2 = df_m2[df_m2.target_label != 'TIE BREAK']\n",
    "df_m2 = df_m2[df_m2.target_label != 5]\n",
    "df_m2['target_label'] = df_m2['target_label'].fillna('')\n",
    "df_m2 = df_m2[df_m2.target_label != '']\n",
    "\n",
    "print(df_m2.target_label.value_counts().sort_index(axis=0))\n",
    "\n",
    "print(\"Total samples: \", df_m2.target_label.value_counts().sum())\n",
    "print(\"DF Shape: \",df_m2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping class 4\n",
    "df_m2 = df_m2[df_m2.target_label != 4]\n",
    "df_m2.target_label.value_counts().sort_index(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#undersample class 0\n",
    "df_m2 = undersample_majority_classes(df_m2, m2_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_m2.X\n",
    "X = clean_X(X)\n",
    "y = df_m2['target_label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dump_path = model_path + '\\m2'\n",
    "if not os.path.exists(model_dump_path):\n",
    "    os.mkdir(model_dump_path)\n",
    "    \n",
    "start_time = time.time()\n",
    "\n",
    "df_class, undersampling_est = best_model_search(X, y, bow, tfidf, model_params,model_dump_path)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "elapsed_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vec in ['bag-of-words','tfidf']:\n",
    "    df_score = df_class[df_class['vect'] == vec]\n",
    "    plot_histogram_data(df_score)\n",
    "    df_score.to_excel(model_dump_path + '/scores_' + vec + 'df_score_m2.xlsx',encoding = 'utf-8-sig')\n",
    "    plt.savefig(model_dump_path + '/fig_' + vec + '_results_m2.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ft = bow.fit_transform(X).toarray()\n",
    "y_pred = cross_val_predict(undersampling_est[1], X_ft, y, cv=10)\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "ax = plt.axes()\n",
    "df_cm = sb.heatmap(pd.DataFrame(cm),cmap='Blues',annot=True,ax=ax)\n",
    "ax.set_title('Best Model - Model 2')\n",
    "plt.savefig(model_dump_path + '/fig_' + vec + '_results_m2_CM.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Model 3\n",
    "##### Converting class 4 into class 1, random undersampling of class 0 (four classes in total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m3 = annotated_data\n",
    "df_m3.target_label.astype(str)\n",
    "df_m3 = df_m3[df_m3.target_label != 'TIE BREAK']\n",
    "df_m3 = df_m3[df_m3.target_label != 5]\n",
    "df_m3['target_label'] = df_m3['target_label'].fillna('')\n",
    "df_m3 = df_m3[df_m3.target_label != '']\n",
    "\n",
    "print(df_m3.target_label.value_counts().sort_index(axis=0))\n",
    "\n",
    "print(\"Total samples: \", df_m3.target_label.value_counts().sum())\n",
    "print(\"DF Shape: \",df_m3.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging class 4 to class 1\n",
    "df_m3['target_label'].replace({4: 1}, inplace=True)\n",
    "df_m3.target_label.value_counts().sort_index(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#undersample class 0\n",
    "df_m3 = undersample_majority_classes(df_m3, m3_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_m3.X\n",
    "X = clean_X(X)\n",
    "y = df_m3['target_label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dump_path = model_path + '\\m3'\n",
    "if not os.path.exists(model_dump_path):\n",
    "    os.mkdir(model_dump_path)\n",
    "start_time = time.time()\n",
    "\n",
    "df_class, undersampling_est = best_model_search(X, y, bow, tfidf, model_params,model_dump_path)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "elapsed_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vec in ['bag-of-words','tfidf']:\n",
    "    df_score = df_class[df_class['vect'] == vec]\n",
    "    plot_histogram_data(df_score)\n",
    "    df_score.to_excel(model_dump_path + '/scores_' + vec + 'df_score_m3.xlsx',encoding = 'utf-8-sig')\n",
    "    plt.savefig(model_dump_path + '/fig_' + vec + '_results_m3.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ft = bow.fit_transform(X).toarray()\n",
    "y_pred = cross_val_predict(undersampling_est[1], X_ft, y, cv=10)\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "ax = plt.axes()\n",
    "df_cm = sb.heatmap(pd.DataFrame(cm),cmap='Blues',annot=True,ax=ax)\n",
    "ax.set_title('Best Model - Model 3')\n",
    "plt.savefig(model_dump_path + '/fig_' + vec + '_results_m3_CM.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Model 4\n",
    "##### Removing class 4, converting class 3 into class 2 (three classes in total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m4 = annotated_data\n",
    "df_m4.target_label.astype(str)\n",
    "df_m4 = df_m4[df_m4.target_label != 'TIE BREAK']\n",
    "df_m4 = df_m4[df_m4.target_label != 5]\n",
    "df_m4['target_label'] = df_m4['target_label'].fillna('')\n",
    "df_m4 = df_m4[df_m4.target_label != '']\n",
    "\n",
    "print(df_m4.target_label.value_counts().sort_index(axis=0))\n",
    "\n",
    "print(\"Total samples: \", df_m4.target_label.value_counts().sum())\n",
    "print(\"DF Shape: \",df_m4.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop class 4\n",
    "df_m4 = df_m4[df_m4.target_label != 4]\n",
    "df_m4.target_label.value_counts().sort_index(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge class 3 into class 2\n",
    "df_m4.target_label.replace(to_replace =3, value = 2,inplace=True)\n",
    "df_m4.target_label.value_counts().sort_index(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#undersample class 0\n",
    "df_m4 = undersample_majority_classes(df_m4, m4_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_m4.X\n",
    "X = clean_X(X)\n",
    "y = df_m4['target_label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dump_path = model_path + '\\m4'\n",
    "if not os.path.exists(model_dump_path):\n",
    "    os.mkdir(model_dump_path)\n",
    "    \n",
    "start_time = time.time()\n",
    "\n",
    "df_class, undersampling_est = best_model_search(X, y, bow, tfidf, model_params,model_dump_path)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "elapsed_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vec in ['bag-of-words','tfidf']:\n",
    "    df_score = df_class[df_class['vect'] == vec]\n",
    "    plot_histogram_data(df_score)\n",
    "    df_score.to_excel(model_dump_path + '/scores_' + vec + 'df_score_m4.xlsx',encoding = 'utf-8-sig')\n",
    "    plt.savefig(model_dump_path + '/fig_' + vec + '_results_m4.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ft = bow.fit_transform(X).toarray()\n",
    "y_pred = cross_val_predict(undersampling_est[1], X_ft, y, cv=10)\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "ax = plt.axes()\n",
    "df_cm = sb.heatmap(pd.DataFrame(cm),cmap='Blues',annot=True,ax=ax)\n",
    "ax.set_title('Best Model - Model 4')\n",
    "plt.savefig(model_dump_path + '/fig_' + vec + '_results_m4_CM.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Model 5\n",
    "##### Converting class 4 to class 1, converting class 3 to 2 (three classes in total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m5 = annotated_data\n",
    "df_m5.target_label.astype(str)\n",
    "df_m5 = df_m5[df_m5.target_label != 'TIE BREAK']\n",
    "df_m5 = df_m5[df_m5.target_label != 5]\n",
    "df_m5['target_label'] = df_m5['target_label'].fillna('')\n",
    "df_m5 = df_m5[df_m5.target_label != '']\n",
    "\n",
    "print(df_m5.target_label.value_counts().sort_index(axis=0))\n",
    "\n",
    "print(\"Total samples: \", df_m5.target_label.value_counts().sum())\n",
    "print(\"DF Shape: \",df_m5.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging class 4 to class 1\n",
    "df_m5.target_label.replace(to_replace =4, value = 1,inplace=True)\n",
    "df_m5.target_label.replace(to_replace =3, value = 2,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m5.target_label.value_counts().sort_index(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#undersample class 0\n",
    "df_m5 = undersample_majority_classes(df_m5, m5_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_m5.X\n",
    "X = clean_X(X)\n",
    "y = df_m5['target_label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dump_path = model_path + '\\m5'\n",
    "if not os.path.exists(model_dump_path):\n",
    "    os.mkdir(model_dump_path)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "df_class, undersampling_est = best_model_search(X, y, bow, tfidf, model_params,model_dump_path)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "elapsed_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vec in ['bag-of-words','tfidf']:\n",
    "    df_score = df_class[df_class['vect'] == vec]\n",
    "    plot_histogram_data(df_score)\n",
    "    df_score.to_excel(model_dump_path + '/scores_' + vec + 'df_score_m5.xlsx',encoding = 'utf-8-sig')\n",
    "    plt.savefig(model_dump_path + '/fig_' + vec + '_results_m5.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ft = bow.fit_transform(X).toarray()\n",
    "y_pred = cross_val_predict(undersampling_est[1], X_ft, y, cv=10)\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "ax = plt.axes()\n",
    "df_cm = sb.heatmap(pd.DataFrame(cm),cmap='Blues',annot=True,ax=ax)\n",
    "ax.set_title('Best Model - Model 5')\n",
    "plt.savefig(model_dump_path + '/fig_' + vec + '_results_m5_CM.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_elapsed_time = time.time() - global_start_time\n",
    "print(global_elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Predictions\n",
    " <b> USER INPUT IS REQUIRED</b> \n",
    "This section is for applying the best model's classifier to predict label for the entire dataset.\n",
    "For the purpose of this project, we decided to limit it to 4 classes - specifically <b>model 3's </b> classifier with the best F1 Score was used which is <b> Logistic Regression </b> </b> using <b> TFIDF </b> with the following params: <b> {'C': 1}.</b>\n",
    "We leave it to the user to identify the best model and change the file path as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(best_model, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard_data = annotated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dashboard_data.X\n",
    "X = clean_X(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ft = tfidf.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = loaded_model.predict(X_ft)\n",
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard_data['y_pred'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard_data.drop(columns=['X'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard_data.to_excel(dashboard_file_path + \"/dashboard_data_predictions.xlsx\",encoding = 'utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
